diff -pruN a/include/linux/init_task.h b/include/linux/init_task.h
--- a/include/linux/init_task.h	2013-03-14 19:27:14.000000000 +0100
+++ b/include/linux/init_task.h	2013-03-19 20:53:39.703208122 +0100
@@ -168,6 +168,9 @@ extern struct task_group root_task_group
 		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
 		.time_slice	= RR_TIMESLICE,				\
 	},								\
+	.dummy_se	= {						\
+		.run_list	= LIST_HEAD_INIT(tsk.dummy_se.run_list),\
+	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	INIT_PUSHABLE_TASKS(tsk)					\
 	INIT_CGROUP_SCHED(tsk)						\
diff -pruN a/include/linux/sched.h b/include/linux/sched.h
--- a/include/linux/sched.h	2013-03-14 19:27:14.000000000 +0100
+++ b/include/linux/sched.h	2013-03-19 20:53:39.943208122 +0100
@@ -1226,6 +1226,10 @@ struct sched_rt_entity {
  */
 #define RR_TIMESLICE		(100 * HZ / 1000)
 
+struct sched_dummy_entity {
+	struct list_head run_list;
+};
+
 struct rcu_node;
 
 enum perf_event_task_context {
@@ -1253,6 +1257,7 @@ struct task_struct {
 	const struct sched_class *sched_class;
 	struct sched_entity se;
 	struct sched_rt_entity rt;
+	struct sched_dummy_entity dummy_se;
 #ifdef CONFIG_CGROUP_SCHED
 	struct task_group *sched_task_group;
 #endif
@@ -1653,6 +1658,20 @@ static inline int rt_task(struct task_st
 	return rt_prio(p->prio);
 }
 
+#define MIN_DUMMY_PRIO          (MAX_RT_PRIO + 35)
+
+static inline int dummy_prio(int prio)
+{
+	if (prio >= MIN_DUMMY_PRIO)
+		return 1;
+	return 0;
+}
+
+static inline int dummy_task(struct task_struct *p)
+{
+	return dummy_prio(p->prio);
+}
+
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
@@ -2038,6 +2057,9 @@ extern unsigned int sysctl_sched_min_gra
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
 
+extern unsigned int sysctl_sched_dummy_timeslice;
+extern unsigned int sysctl_sched_dummy_age_threshold;
+
 enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_NONE,
 	SCHED_TUNABLESCALING_LOG,
diff -pruN a/kernel/sched/core.c b/kernel/sched/core.c
--- a/kernel/sched/core.c	2013-03-14 19:27:14.000000000 +0100
+++ b/kernel/sched/core.c	2013-03-19 20:53:48.663208122 +0100
@@ -1646,8 +1646,14 @@ void sched_fork(struct task_struct *p)
 		p->sched_reset_on_fork = 0;
 	}
 
-	if (!rt_prio(p->prio))
-		p->sched_class = &fair_sched_class;
+	if (!rt_prio(p->prio)) {
+		if (dummy_prio(p->prio)) {
+			p->sched_class = &dummy_sched_class;
+		} else {
+			p->sched_class = &fair_sched_class;
+		}
+	}
+
 
 	if (p->sched_class->task_fork)
 		p->sched_class->task_fork(p);
@@ -2828,11 +2834,11 @@ pick_next_task(struct rq *rq)
 	 * Optimization: we know that if all tasks are in
 	 * the fair class we can call that function directly:
 	 */
-	if (likely(rq->nr_running == rq->cfs.h_nr_running)) {
-		p = fair_sched_class.pick_next_task(rq);
-		if (likely(p))
-			return p;
-	}
+	//if (likely(rq->nr_running == rq->cfs.h_nr_running)) {
+	//	p = fair_sched_class.pick_next_task(rq);
+	//	if (likely(p))
+	//		return p;
+	//}
 
 	for_each_class(class) {
 		p = class->pick_next_task(rq);
@@ -3557,10 +3563,13 @@ void rt_mutex_setprio(struct task_struct
 	if (running)
 		p->sched_class->put_prev_task(rq, p);
 
-	if (rt_prio(prio))
+	if (rt_prio(prio)) {
 		p->sched_class = &rt_sched_class;
-	else
+	} else if (dummy_prio(prio)) {
+		p->sched_class = &dummy_sched_class;
+	} else {
 		p->sched_class = &fair_sched_class;
+	}
 
 	p->prio = prio;
 
@@ -3607,6 +3616,10 @@ void set_user_nice(struct task_struct *p
 	p->prio = effective_prio(p);
 	delta = p->prio - old_prio;
 
+	if (dummy_prio(p->prio)) {
+		p->sched_class = &dummy_sched_class;
+	}
+
 	if (on_rq) {
 		enqueue_task(rq, p, 0);
 		/*
@@ -3749,10 +3762,13 @@ __setscheduler(struct rq *rq, struct tas
 	p->normal_prio = normal_prio(p);
 	/* we are holding p->pi_lock already */
 	p->prio = rt_mutex_getprio(p);
-	if (rt_prio(p->prio))
+	if (rt_prio(p->prio)) {
 		p->sched_class = &rt_sched_class;
-	else
+	} else if (dummy_prio(p->prio)) {
+		p->sched_class = &dummy_sched_class;
+	} else {
 		p->sched_class = &fair_sched_class;
+	}
 	set_load_weight(p);
 }
 
@@ -6893,6 +6909,7 @@ void __init sched_init(void)
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt, rq);
+		init_dummy_rq(&rq->dummy, rq);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
@@ -6983,7 +7000,11 @@ void __init sched_init(void)
 	/*
 	 * During early bootup we pretend to be a normal task:
 	 */
-	current->sched_class = &fair_sched_class;
+	if (unlikely(current->prio >= MIN_DUMMY_PRIO)) {
+		current->sched_class = &dummy_sched_class;
+	} else {
+		current->sched_class = &fair_sched_class;
+	}
 
 #ifdef CONFIG_SMP
 	zalloc_cpumask_var(&sched_domains_tmpmask, GFP_NOWAIT);
diff -pruN a/kernel/sched/dummy.c b/kernel/sched/dummy.c
--- a/kernel/sched/dummy.c	1970-01-01 01:00:00.000000000 +0100
+++ b/kernel/sched/dummy.c	2013-03-19 20:53:48.663208122 +0100
@@ -0,0 +1,148 @@
+/*
+ * Dummy scheduling class, mapped to last five levels of SCHED_NORMAL policy
+ */
+
+#include "sched.h"
+
+/*
+ * Timeslice and age threshold are represented in jiffies. Default timeslice
+ * is 100ms. Both parameters can be tuned from /proc/sys/kernel.
+ */
+
+#define DUMMY_TIMESLICE		(100 * HZ / 1000)
+#define DUMMY_AGE_THRESHOLD	(3 * DUMMY_TIMESLICE)
+
+unsigned int sysctl_sched_dummy_timeslice = DUMMY_TIMESLICE;
+static inline unsigned int get_timeslice()
+{
+	return sysctl_sched_dummy_timeslice;
+}
+
+unsigned int sysctl_sched_dummy_age_threshold = DUMMY_AGE_THRESHOLD;
+static inline unsigned int get_age_threshold()
+{
+	return sysctl_sched_dummy_age_threshold;
+}
+
+/*
+ * Init
+ */
+
+void init_dummy_rq(struct dummy_rq *dummy_rq, struct rq *rq)
+{
+	INIT_LIST_HEAD(&dummy_rq->queue);
+}
+
+/*
+ * Helper functions
+ */
+
+static inline struct task_struct *dummy_task_of(struct sched_dummy_entity *dummy_se)
+{
+	return container_of(dummy_se, struct task_struct, dummy_se);
+}
+
+static inline void _enqueue_task_dummy(struct rq *rq, struct task_struct *p)
+{
+	struct sched_dummy_entity *dummy_se = &p->dummy_se;
+        struct list_head *queue = &rq->dummy.queue;
+        list_add_tail(&dummy_se->run_list, queue);
+}
+
+static inline void _dequeue_task_dummy(struct task_struct *p)
+{
+	struct sched_dummy_entity *dummy_se = &p->dummy_se;
+        list_del_init(&dummy_se->run_list);
+}
+
+/*
+ * Scheduling class functions to implement
+ */
+
+static void enqueue_task_dummy(struct rq *rq, struct task_struct *p, int flags)
+{
+	_enqueue_task_dummy(rq, p);
+	inc_nr_running(rq);
+}
+
+static void dequeue_task_dummy(struct rq *rq, struct task_struct *p, int flags)
+{
+	_dequeue_task_dummy(p);
+	dec_nr_running(rq);
+}
+
+static void yield_task_dummy(struct rq *rq)
+{
+}
+
+static void check_preempt_curr_dummy(struct rq *rq, struct task_struct *p, int flags)
+{
+}
+
+static struct task_struct *pick_next_task_dummy(struct rq *rq)
+{
+	struct dummy_rq *dummy_rq = &rq->dummy;
+	struct sched_dummy_entity *next;
+
+	if (!list_empty(&dummy_rq->queue)) {
+		next = list_first_entry(&dummy_rq->queue, struct sched_dummy_entity, run_list);
+		return dummy_task_of(next);
+	} else {
+		return NULL;
+	}
+}
+
+static void put_prev_task_dummy(struct rq *rq, struct task_struct *prev)
+{
+}
+
+static void set_curr_task_dummy(struct rq *rq)
+{
+}
+
+static void task_tick_dummy(struct rq *rq, struct task_struct *curr, int queued)
+{
+}
+
+static void switched_from_dummy(struct rq *rq, struct task_struct *p)
+{
+}
+
+static void switched_to_dummy(struct rq *rq, struct task_struct *p)
+{
+}
+
+static void prio_changed_dummy(struct rq *rq, struct task_struct *p, int oldprio)
+{
+}
+
+static unsigned int get_rr_interval_dummy(struct rq *rq, struct task_struct *p)
+{
+	return get_timeslice();
+}
+
+/*
+ * Scheduling class
+ */
+
+const struct sched_class dummy_sched_class = {
+	.next		    = &idle_sched_class,
+	.enqueue_task       = enqueue_task_dummy,
+	.dequeue_task       = dequeue_task_dummy,
+	.yield_task         = yield_task_dummy,
+
+	.check_preempt_curr = check_preempt_curr_dummy,
+
+	.pick_next_task     = pick_next_task_dummy,
+	.put_prev_task      = put_prev_task_dummy,
+
+	.set_curr_task      = set_curr_task_dummy,
+	.task_tick          = task_tick_dummy,
+
+	.switched_from      = switched_from_dummy,
+	.switched_to        = switched_to_dummy,
+	.prio_changed       = prio_changed_dummy,
+
+	.get_rr_interval    = get_rr_interval_dummy,
+};
+
diff -pruN a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c	2013-03-14 19:27:14.000000000 +0100
+++ b/kernel/sched/fair.c	2013-03-19 20:53:48.663208122 +0100
@@ -6110,7 +6110,7 @@ static unsigned int get_rr_interval_fair
  * All the scheduling class methods:
  */
 const struct sched_class fair_sched_class = {
-	.next			= &idle_sched_class,
+	.next			= &dummy_sched_class,
 	.enqueue_task		= enqueue_task_fair,
 	.dequeue_task		= dequeue_task_fair,
 	.yield_task		= yield_task_fair,
diff -pruN a/kernel/sched/Makefile b/kernel/sched/Makefile
--- a/kernel/sched/Makefile	2013-03-14 19:27:14.000000000 +0100
+++ b/kernel/sched/Makefile	2013-03-19 20:53:48.663208122 +0100
@@ -11,7 +11,7 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o clock.o cputime.o idle_task.o fair.o rt.o stop_task.o
+obj-y += core.o clock.o cputime.o idle_task.o fair.o rt.o stop_task.o dummy.o
 obj-$(CONFIG_SMP) += cpupri.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
diff -pruN a/kernel/sched/sched.h b/kernel/sched/sched.h
--- a/kernel/sched/sched.h	2013-03-14 19:27:14.000000000 +0100
+++ b/kernel/sched/sched.h	2013-03-19 20:53:48.663208122 +0100
@@ -79,6 +79,7 @@ extern struct mutex sched_domains_mutex;
 
 struct cfs_rq;
 struct rt_rq;
+struct dummy_rq;
 
 extern struct list_head task_groups;
 
@@ -322,6 +323,10 @@ struct rt_rq {
 #endif
 };
 
+struct dummy_rq {
+	struct list_head queue;
+};
+
 #ifdef CONFIG_SMP
 
 /*
@@ -383,6 +388,7 @@ struct rq {
 
 	struct cfs_rq cfs;
 	struct rt_rq rt;
+	struct dummy_rq dummy;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
@@ -870,6 +876,7 @@ enum cpuacct_stat_index {
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
+extern const struct sched_class dummy_sched_class;
 extern const struct sched_class idle_sched_class;
 
 
@@ -1178,6 +1185,7 @@ extern void print_rt_stats(struct seq_fi
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+extern void init_dummy_rq(struct dummy_rq *dummy_rq, struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
 
diff -pruN a/kernel/sysctl.c b/kernel/sysctl.c
--- a/kernel/sysctl.c	2013-03-14 19:27:14.000000000 +0100
+++ b/kernel/sysctl.c	2013-03-19 20:53:48.673208122 +0100
@@ -303,6 +303,20 @@ static struct ctl_table kern_table[] = {
 		.extra1		= &min_wakeup_granularity_ns,
 		.extra2		= &max_wakeup_granularity_ns,
 	},
+	{
+		.procname	= "sched_dummy_timeslice",
+		.data		= &sysctl_sched_dummy_timeslice,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec
+	},
+	{
+		.procname	= "sched_dummy_age_threshold",
+		.data		= &sysctl_sched_dummy_age_threshold,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec
+	},
 #ifdef CONFIG_SMP
 	{
 		.procname	= "sched_tunable_scaling",
